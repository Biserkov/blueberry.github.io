<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title><font color = "OrangeRed">Bayadera</font><br/> <font color = "red">Bayes</font> + <font color = "lime">Clojure</font> + <font color = "yellow">GPU</font></title>
<meta name="author" content="(Dragan Djuric)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/css/reveal.css"/>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/css/theme/black.css" id="theme"/>

<link rel="stylesheet" href="noborder.css"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h2><font color = "OrangeRed">Bayadera</font><br/> <font color = "red">Bayes</font> + <font color = "lime">Clojure</font> + <font color = "yellow">GPU</font></h2><h3>Dragan Djuric</h3><strong><a href="mailto:dragandj@gmail.com">dragandj@gmail.com</a></strong>
</section>
<aside class="notes">
<p>
Hello. This is a presentation of Bayadera. It's a Bayesian statistics and machine learning library,
implemented in Clojure, that get its speed from GPU's.
</p>

</aside>
<section>
<section id="slide-orgheadline1">
<h2 id="orgheadline1">Dragan Djuric</h2>
<ul>
<li>blog <a href="http://dragan.rocks">http://dragan.rocks</a></li>
<li>twitter <a href="https://twitter.com/draganrocks">@draganrocks</a></li>
<li>Professor of Software Engineering</li>
<li>University of Belgrade</li>
<li>Clojure as a primary language since <b>2009</b></li>
<li>Interested in Bayesian methods in ML.</li>
<li>dragandj@gmail.com</li>
<li>github: blueberry / uncomplicate</li>
<li><a href="http://uncomplicate.org">http://uncomplicate.org</a></li>

</ul>

<aside class="notes">
<p>
My name is Dragan Djuric, I am a professor of software engineering at
the University of Belgrade.
</p>

<p>
 I've been using Clojure as my primary programming language since 2009,
and I teach it since 2010.
</p>

<p>
I am interested in ML, especially the bayesian kind.
</p>

<p>
You can find me on github, or read my blog at dragan.rocks, where you can also find this presentation.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgheadline8">
<h2 id="orgheadline8">A little background</h2>
<ul>
<li>Clojure</li>
<li>GPU computing</li>
<li>Bayesian Data Analysis</li>

</ul>

<aside class="notes">
<p>
There are 3 topics in this talk:
</p>
<ul>
<li>Clojure</li>
<li>GPU computing</li>
<li>Bayesian Data Analysis</li>

</ul>

</aside>
</section>
<section id="slide-orgheadline2">
<h3 id="orgheadline2">Clojure - a modern lisp</h3>
<ul>
<li>Dynamic <b>and</b> fast</li>
<li>First-class functions</li>
<li>Great abstractions and data structures</li>
<li>Many useful libraries</li>
<li>Even more experimental libraries</li>
<li>Seamless access to Java and the JVM</li>
<li>ClojureScript in browsers, node etc.</li>

</ul>


<aside class="notes">
<p>
Clojure is great! It's a modern, pragmatical lisp, that integrates well into today's most popular platforms.
</p>

</aside>

</section>
<section id="slide-orgheadline3">
<h3 id="orgheadline3">Are Java and Clojure good at number crunching?</h3>
<ul>
<li>Good? maybe.</li>
<li>Great? NO!</li>
<li>JVM - no access to hardware-specific optimizations</li>

<li>We can make it great!</li>

</ul>

<aside class="notes">
<p>
But, is it good at number crunching?
</p>

<p>
It is certainly not bad, but compared to what's available elsewhere, it is not great either.
</p>

<p>
One of my goals is to make Clojure great for implementing numerical-heavy algorithms.
</p>

</aside>

</section>
<section id="slide-orgheadline4">
<h3 id="orgheadline4">CPU is not so great either!</h3>
<ul>
<li>R, Python? Even worse than Java.</li>
<li>C? complicated, verbose platform-specific optimizations.</li>
<li>CPU? too beefed-up. Burns as the Sun!</li>

</ul>

<aside class="notes">
<p>
What about other managed platforms? Those are even worse than JVM when it comes
to number crunching. It is true that they have rich libraries that fall back
to native code, but&#x2026; writing <b>custom</b> algorithms is very difficult&#x2026;
</p>

<p>
And CPU itself is not so great either! CPUs have rich instruction sets, but
that requires lots of transistors that devours energy.
</p>

</aside>

</section>
<section id="slide-orgheadline5">
<h3 id="orgheadline5">GPU has a lot to offer &#x2026;at a price</h3>
<ul>
<li>many dumb computing units</li>
<li>but, power-efficient for number crunching</li>
<li>hardware support for massive parallelism</li>
<li>faster and cheaper each year</li>
<li><b>notoriously difficult to program</b></li>

</ul>

<aside class="notes">
<p>
&#x2026;which brings us to GPUs. GPUs contain very simple computing units, but it is
possible to pack many of those on one chip!
The features that are provided are exactly the numerical operations and parallelization
support that we need! They are more powerful and cheaper each year.
It's a shame that the programming platforms are from the stone age!
</p>

</aside>

</section>
<section id="slide-orgheadline6">
<h3 id="orgheadline6">Uncomplicate</h3>
<dl>
<dt><font color = "yellow">Fluokitten</font></dt><dd>fluorescent monadic fuzzy little things</dd>
<dt><font color = "yellow">ClojureCL</font></dt><dd>take control of the GPU and CPU from Clojure</dd>
<dt><font color = "yellow">Neanderthal</font></dt><dd>optimized vectors and matrices on CPU and GPU</dd>
<dt><font color = "OrangeRed">Bayadera</font></dt><dd>high performance Bayesian statistics and data analysis on the GPU</dd>

</dl>

<aside class="notes">
<p>
Uncomplicate is a family of open-source Clojure libraries for scientific and high-performance computing.
</p>

<p>
My main goal was to create a state-of-the art Bayesian machine learning infrastructure. For that to fly,
I also had to provide lower-level libraries, since what was available at the time on JVM, was lot worse
than the best tools available elsewhere.
</p>

<p>
ClojureCL is a Clojure library for taking <b>full</b> control of the GPU with minimal overhead.
</p>

<p>
Neanderthal leverages battle-tested native linear algebra libraries for the CPU and latest GPU libraries
to provide the full-speed matrix support tailored to Clojure with almost no overhead.
</p>

<p>
Bayadera goes beyond that - a library for Bayesian statistics on the GPU that is much faster than
the fastest available mainstream offerings!
</p>

</aside>

</section>
<section id="slide-orgheadline7">
<h3 id="orgheadline7">What does all this has to do with Bayes?</h3>
<ul>
<li>BDA is conceptually simple</li>
<li>Real models must be calculated numerically</li>
<li>MCMC sampling and related methods</li>
<li>In higher dimensions, requires huge samples</li>
<li>Analyses typically run in minutes, hours, weeks</li>

</ul>
<p>
.
</p>
<ul>
<li>Good: Clojure and GPU FTW</li>
<li>Challenge: <b>MCMC is sequential</b></li>

</ul>

<aside class="notes">
<p>
Bayesian methods are conceptually simple and elegant (although not always easy to learn).
</p>

<p>
The main challenge is that <b>real-world</b> models are not possible to solve analytically, and require
<b>enormous</b> computational resources. Main numerical methods are variants of MCMC simulation, and analyses
typically run in minutes, hours, days, weeks or even more.
</p>

<p>
GPU are an obvious help. Numerical methods can typically be fast on GPU's.
</p>

<p>
But there's a challenge: <b>MCMC is inherently sequential</b> so it is difficult to parallelize.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgheadline15">
<h2 id="orgheadline15">A Bayesian Data Analysis Primer</h2>
<p>
How to know something we cannot observe?
</p>

<ul>
<li>Probabilities of all answers (<b>prior</b>)</li>
<li>Probability of measured data (<b>evidence</b> and <b>likelihood</b>)</li>
<li>Calculate "backwards", using Bayes' rule and get</li>
<li><b>posterior</b> probabilities of answers</li>

</ul>
<div>
\begin{equation*}
\Pr(H|D) = \frac{\Pr(D|H)\times \Pr(H)}{\Pr(D)}
\end{equation*}

</div>

<aside class="notes">
<p>
When could BDA be useful? Whenever we have too little data and too much uncertainty.
</p>

<p>
Often we cannot directly measure what we are interested in.
</p>

<p>
We can only measure something <b>else</b>, that we think influences it.
There is <b>rarely</b> one true answer. More likely, possible answers are more or less probable.
</p>

<p>
BDA helps us discover the probability of each potential answer by taking into
account what we knew previously (<b>prior</b>), our knowledge of the random process itself (<b>likelihoodh</b>),
and what we measured in the data (<b>evience</b>). We do that calculation using Bayes' rule.
</p>

</aside>
</section>
<section id="slide-orgheadline9">
<h3 id="orgheadline9">Doing Bayesian Data Analysis</h3>
<ul>
<li>John Kruschke</li>
<li>Excellent tutorial, even for beginners</li>
<li>Real-world examples through the book!</li>

</ul>


<div class="figure">
<p><img src="dbda-book.png" alt="dbda-book.png" height="30%," width="30%" />
</p>
</div>

<aside class="notes">
<p>
This book is what I would recommend as the best learning resource. It is approachable by (strong) beginners,
but the examples are still from the real world analysis.
</p>

</aside>

</section>
<section id="slide-orgheadline10">
<h3 id="orgheadline10">Hello World: Fair or Trick coin?</h3>
<ul>
<li>Coin bias, tendency to fall on head: &theta;</li>
<li>We can <b>not</b> measure &theta; directly</li>
<li>We can only know sample data (D)</li>
<li>&theta; is general; &theta;<sub>1</sub>, &theta;<sub>2</sub>, &#x2026;, &theta;<sub>n</sub> are specific values</li>
<li>D is general; 3 heads out of 10 specific</li>

</ul>

<p>
By Bayes' rule:
</p>

<div>
\begin{equation*}
\Pr(\theta|D) = \frac{\Pr(D|\theta)\times \Pr(\theta)}{\Pr(D)}
\end{equation*}

</div>

<aside class="notes">
<p>
Since we are programmers, we would like to start with the Hello World! In BDA, hello world example
is the coin throwing experiment.
</p>

<p>
We are interested in the coin's "fairness", the tendency to fall on the head, or tale. We describe
that with the parameter &theta;.
</p>

<p>
Unfortunately, &theta; is not directly accessible. However, we can make experimet, in which we know
that the process of coin throwing have binomial <b>likelihood</b>. We measure outcomes, that we know
(by the construction of the experiment) are influenced by &theta;, - we gather <b>evidence</b>.
</p>

<p>
Using Bayes' rule, posterior = likelihood &times; prior divided by evidence.
</p>

</aside>

</section>
<section id="slide-orgheadline11">
<h3 id="orgheadline11">Analytical solution</h3>
<p>
Likelihood:
</p>
<p>
Pr(z,N|&theta;) = &theta;<sup>z</sup> (1 - &theta;)<sup>N-z</sup>
</p>

<p>
(conjugate) prior
</p>
<p>
Pr(&theta;|a,b) = beta(&theta;|a,b) = &theta;<sup>(a-1)</sup>(1-&theta;)<sup>(b-1)</sup>/B(a,b)
</p>

<p>
Posterior, conveniently:
</p>
<p>
Pr(&theta;|z,N) = beta(&theta;|z+a, N - z + b)
</p>

<aside class="notes">
<p>
Fortunately, for this simple setup, we can calculate the posterior <b>analytically</b>, and the
result is a general formula.
</p>

<p>
By the design of the experiment, likelihood is binomial.
</p>

<p>
Then, if we conveniently choose a prior that "fits" with this likelihood, we can easily calculate
the general formula for the posterior distribution of probabilities.
</p>

<p>
In this case, we choose  the Beta distribution, which is a conjugate prior of binomial likelihood,
and give another Beta distribution as the answer.
</p>

</aside>

</section>
<section id="slide-orgheadline12">
<h3 id="orgheadline12">Example from Kruschke's book</h3>

<div class="figure">
<p><img src="coin-analytical.png" alt="coin-analytical.png" height="50%," width="50%" />
</p>
</div>
<aside class="notes">
<p>
Kruschke's book gives an example with specific numbers. Fe first believe that the coin is most
probably fair (&theta; around 0.5), but we are not very convinced that it is. That prior is in the first diagram.
</p>

<p>
On the middle diagram, we see what the likelihood of the actual data, 1 head out of 10 flips is, <b>for each possible &theta;</b>
If just one head comes out of 10 throws, it is much more likely that theta is, say. 0.1 or 0.18 than 7.45. However,
7.65 is still somewhat likely, just much less likely than 0.1.
</p>

<p>
So, do we trust our prior knowledge, or the specific data outcome?
</p>

<p>
Actually, <b>both</b>. The posterior is actually a compromise that includes both prior knowledge and the
actual data that happened.
</p>

</aside>
</section>
<section id="slide-orgheadline13">
<h3 id="orgheadline13">Easy computation</h3>
<p>
Q: What is Pr(0.4&lt;&theta;&lt;0.6)?
</p>

<p>
A: Pr(&theta;&lt;0.6) - Pr(&theta;&lt;0.4)
</p>


<aside class="notes">
<p>
Since we have analytical solution for this model, it is very easy to quickly get the answer when we
plug in the specific numbers.
</p>

<p>
In this case, we consider the coin somewhat fair if the &theta; is "around" 0.5. Arbitrarily, we set the
wide bounds of 0.4 - 0.6. The probability of that range can be calculated by computing <b>cumulate probability</b>.
</p>

<p>
Bayadera and other statistical software usually have CDF functions of all common probability distributions.
</p>

<p>
Result: 0.15. It is possible that this coin is fair, but I wouldn't bet on it.
</p>

</aside>
<div class="org-src-container">

<pre  class="src src-clojure"><span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">let</span> <span style="color: #006400;">[</span>a-prior 5
      b-prior 5
      z 1
      <span style="color: #96CBFE;">N</span> 10
      a-post <span style="color: #ff1493;">(</span>+ a-prior z<span style="color: #ff1493;">)</span>
      b-post <span style="color: #ff1493;">(</span>+ b-prior <span style="color: #ffff00;">(</span>- <span style="color: #96CBFE;">N</span> z<span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span><span style="color: #006400;">]</span>

<span style="color: #006400;">(</span>- <span style="color: #ff1493;">(</span>beta-cdf a-post b-post 0.6<span style="color: #ff1493;">)</span>
   <span style="color: #ff1493;">(</span>beta-cdf a-post b-post 0.4<span style="color: #ff1493;">)</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>
</pre>
</div>

</section>
<section id="slide-orgheadline14">
<h3 id="orgheadline14">Never happens in practice</h3>
<ul>
<li>Only the simplest models are "nice"</li>
<li>The curse of dimensionality</li>

</ul>

<aside class="notes">
<p>
Unfortunately, only the simplest models are such "nice". Usually, we do not have analytical solution.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orgheadline19">
<h2 id="orgheadline19"><font color = "orangered">Bayadera</font></h2>
<ul>
<li>a Clojure library
<ul>
<li>highly opinionated - Bayesian</li>
<li>probabilistic</li>
<li>need to NOT be super slow - thus <b>GPU</b></li>
<li>actually is the fastest I have seen</li>

</ul></li>
<li>use cases:
<ul>
<li>Bayesian data analysis (more stats-oriented)</li>
<li>a foundation for machine learning algorithms</li>
<li>lots of statistical number crunching</li>
<li>risk assessment, decision analysis, etc.</li>

</ul></li>

</ul>

<aside class="notes">
<p>

</p>

<p>
Bayadera is a highly opinionated Bayesian probabilistic Clojure library, that is
much faster than the state-of-the-art established environments.
</p>

<p>
I built it primarily for doing Bayesian data analysis, but plan to
build on it to do machine learning.
</p>

</aside>

</section>
<section id="slide-orgheadline16">
<h3 id="orgheadline16">Bayadera Goals</h3>
<ul>
<li><b>programmers are the first-class citizens</b></li>
<li><b>not</b> a "me too, just in Clojure"</li>
<li><b>different</b> and better (for what I want to do)</li>
<li>for interactive <b>and server</b> use</li>

</ul>

<aside class="notes">
<p>
Bayadera aims at being a well-designed software product.
</p>

<p>
Programmers should feel at home and be able to do both the usual scientific
stuff, but also to build robust software running on servers!
</p>

<p>
It does NOT to try to catch up to what R and Python (and Julia) already do.
</p>

</aside>

</section>
<section id="slide-orgheadline17">
<h3 id="orgheadline17">HARD to compute</h3>
<p>
Usually:
</p>

<div>
\begin{equation*}
\Pr(\vec{h}|\vec{d}) = \frac{\prod_i f(\vec{d_i},\vec{h})\times g(\vec{h})}{\idotsint \prod_i f(\vec{d_i},\vec{h})\,d \vec{h}}
\end{equation*}

</div>

<p>
computationally:
</p>

<div>
\begin{equation*}
answer = \frac{hard\times acceptable}{impossible}
\end{equation*}

</div>

<aside class="notes">
<p>
The main problem that held Bayesian methods dormant for 200 years,
until a decade or two ago was that it is ridiculouslyhard to compute in the general case!
</p>

<p>
Usually, we do not have a nice unimodal one-dimensional problems that nicely fit into
analytical formulas and lead to simple results.
We have to explore this multi-dimensional continuous space numerically.
</p>

<p>
So, to get the answer, we have to multiply something that is hard to compute with
something that is acceptably simpler, do this many (millions, billions) of times, and
divide by something that is practically impossible to compute by brute force.
</p>

</aside>

</section>
<section id="slide-orgheadline18">
<h3 id="orgheadline18">Markov Chain Monte Carlo (MCMC)</h3>
<ul>
<li>a family of simulation algorithms</li>
<li>draws samples from <b>unknown</b> probability distributions</li>
<li>(enough) samples approximate the distribution</li>

</ul>

<div>
\begin{equation*}
\Pr(\vec{h}|\vec{d}) \propto \exp \left(\sum_i \log f(\vec{d_i},\vec{h}) + \log g(\vec{h})\right)
\end{equation*}

</div>

<p>
computationally:
</p>

<div>
\begin{equation*}
answer \propto zillions \times (hard\times acceptable)
\end{equation*}

</div>

<aside class="notes">
<p>
Long story short, MCMC can simulate and draw samples from an unknown distribution.
With enough samples, we can approximate that distribution -
the impossible-to-compute posterior that we were looking for!
</p>

<p>
By using the MCMC algorithm, we are left with "just"
computing zillions of "only" hard to compute things.
</p>

<p>
That's why CPU-based Bayesian tools are notoriously slow. Luckily,
GPU can help us in bringing that time to acceptable levels.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgheadline22">
<h2 id="orgheadline22">The coin example through simulation</h2>
<ul>
<li>MCMC not necessary in this case</li>
<li>Good as a hello world example</li>
<li>Comparison with analytical solution</li>

</ul>

<aside class="notes">
<p>
Now, we will compute the same coin model as before, but now with Bayadera's MCMC engine,
as if we didin't have analytical solution. It is good for illustration, and the bonus is that we can
compare it with the analytical solution and see whether it is correct.
</p>

</aside>
</section>
<section id="slide-orgheadline20">
<h3 id="orgheadline20">Run the analysis on the GPU</h3>
<div class="org-src-container">

<pre  class="src src-clojure"><span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">def</span> <span style="color: #ff69b4; font-weight: bold;">result</span> <span style="color: #006400;">(</span>atom
<span style="color: #ff1493;">(</span>with-default-bayadera
  <span style="color: #ffff00;">(</span><span style="color: #4c83ff; font-weight: bold;">let</span> <span style="color: #00ff00;">[</span>a 5 b 5
        z 1 <span style="color: #96CBFE;">N</span> 10<span style="color: #00ff00;">]</span>
    <span style="color: #00ff00;">(</span>with-release
      <span style="color: #add8e6;">[</span>prior-dist <span style="color: #ffa500;">(</span>beta a b<span style="color: #ffa500;">)</span>
       prior-sampler <span style="color: #ffa500;">(</span>sampler prior-dist<span style="color: #ffa500;">)</span>
       prior-sample <span style="color: #ffa500;">(</span>dataset <span style="color: #6a5acd;">(</span>sample! prior-sampler<span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span>
       prior-pdf <span style="color: #ffa500;">(</span>pdf prior-dist prior-sample<span style="color: #ffa500;">)</span>
       post <span style="color: #ffa500;">(</span>posterior <span style="color: #6a5acd;">(</span>posterior-model <span style="color: #d3d3d3;">(</span><span style="color: #96CBFE;">:binomial</span> likelihoods<span style="color: #d3d3d3;">)</span>
                                        <span style="color: #d3d3d3;">(</span><span style="color: #96CBFE;">:beta</span> distributions<span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span>
       post-dist <span style="color: #ffa500;">(</span>post <span style="color: #6a5acd;">(</span>sv <span style="color: #d3d3d3;">(</span>op <span style="color: #8b0000;">(</span>binomial-lik-params <span style="color: #96CBFE;">N</span> z<span style="color: #8b0000;">)</span> <span style="color: #8b0000;">(</span>beta-params a b<span style="color: #8b0000;">)</span><span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span>
       post-sampler <span style="color: #ffa500;">(</span>time <span style="color: #6a5acd;">(</span><span style="color: #4c83ff; font-weight: bold;">doto</span> <span style="color: #d3d3d3;">(</span>sampler post-dist<span style="color: #d3d3d3;">)</span> <span style="color: #d3d3d3;">(</span>mix!<span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span>
       post-sample <span style="color: #ffa500;">(</span>dataset <span style="color: #6a5acd;">(</span>sample! post-sampler<span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span>
       post-pdf <span style="color: #ffa500;">(</span>scal! <span style="color: #6a5acd;">(</span>/ 1.0 <span style="color: #d3d3d3;">(</span>evidence post-dist prior-sample<span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span>
                       <span style="color: #6a5acd;">(</span>pdf post-dist post-sample<span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span><span style="color: #add8e6;">]</span>

      <span style="color: #add8e6;">{</span><span style="color: #96CBFE;">:prior</span> <span style="color: #ffa500;">{</span><span style="color: #96CBFE;">:sample</span> <span style="color: #6a5acd;">(</span>native <span style="color: #d3d3d3;">(</span>row <span style="color: #8b0000;">(</span><span style="color: #afd8af; font-weight: bold;">p</span><span style="color: #d3d3d3; background-color: #000000;">/</span>data prior-sample<span style="color: #8b0000;">)</span> 0<span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span>
               <span style="color: #96CBFE;">:pdf</span> <span style="color: #6a5acd;">(</span>native prior-pdf<span style="color: #6a5acd;">)</span><span style="color: #ffa500;">}</span>
       <span style="color: #96CBFE;">:posterior</span> <span style="color: #ffa500;">{</span><span style="color: #96CBFE;">:sample</span> <span style="color: #6a5acd;">(</span>native <span style="color: #d3d3d3;">(</span>row <span style="color: #8b0000;">(</span><span style="color: #afd8af; font-weight: bold;">p</span><span style="color: #d3d3d3; background-color: #000000;">/</span>data post-sample<span style="color: #8b0000;">)</span> 0<span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span>
                   <span style="color: #96CBFE;">:pdf</span> <span style="color: #6a5acd;">(</span>native post-pdf<span style="color: #6a5acd;">)</span><span style="color: #ffa500;">}</span><span style="color: #add8e6;">}</span><span style="color: #00ff00;">)</span><span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>
</pre>
</div>

<aside class="notes">
<p>
This listing shows a real code, an I purposefully made it low-level, to
show you important steps in Bayadera architecture. Once you create a model,
you can use all usual Clojure abstractions, to encapsulate most of those details,
so they could be used with a line or two of code.
</p>

<ul>
<li>Distribution</li>
<li>Sampler</li>
<li>Pdf</li>
<li>Models</li>
<li>Posterior (beta * binomial)</li>
<li><b>mix!</b> - running MCMC</li>

</ul>

</aside>

</section>
<section id="slide-orgheadline21">
<h3 id="orgheadline21">Prior and posterior - samples!</h3>

<div class="figure">
<p><img src="coin.png" alt="coin.png" />
</p>
</div>

<aside class="notes">
<p>
So, once we take many thousands of (almost free!) samples, we can simply plot each of those,
and we get perfectly fine-grained plots. Yes, it is an overkill :)
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orgheadline32">
<h2 id="orgheadline32">Real-World: Robust Linear Regression</h2>

<div class="figure">
<p><img src="kruschke-hlr-data.png" alt="kruschke-hlr-data.png" height="50%," width="50%" />
</p>
</div>
<aside class="notes">
<p>
But, it is not an overkill in difficult multidimensional model. There, the more samples we can get, the better.
</p>

<p>
Here is another example from Kruschke's book. We have a sample data of 300 measures of people's height and weight.
</p>

<p>
We would like to know what's correlation between them. Since data obviously does not fall on one line,
we decide that we'll try to find a <b>distribution</b> of all lines that could approximate this linear correlation.
</p>

<p>
Blue lines represent some of the most probable lines, and the noise at each region.
</p>

</aside>
</section>
<section id="slide-orgheadline23">
<h3 id="orgheadline23">Hierarchical Model</h3>

<div class="figure">
<p><img src="kruschke-hlr-model.png" alt="kruschke-hlr-model.png" />
</p>
</div>

<aside class="notes">
<p>
Here's the model.
</p>

<ul>
<li>x(i) is <b>height</b> of person i</li>
<li>y(i) is <b>weight</b> of person i</li>

</ul>

<p>
We suppose that there is a connection x -&gt; y, such that y(i) is t-distributed around &beta;<sub>0</sub> + &beta;<sub>1</sub> * x<sub>i</sub>.
That t distribution <b>also</b> has uncertain parameters &nu; and &sigma;.
</p>

<p>
&nu; is exponentially distributed
&sigma; comes from an uniform distribution
</p>

<p>
&beta;<sub>0</sub> and &beta;<sub>1</sub> are themselves uncertain, they are normally distributed
</p>

<p>
Practically, this model is *4-dimensional: &beta;<sub>0</sub>, beta<sub>1</sub>, &nu; and &sigma; are the 4 dimensions
of that joint random variable, while K, M<sub>0</sub>, S<sub>0</sub>, M<sub>1</sub>, S<sub>1</sub>, L and H are parameters of that 4-dimensional distribution.
</p>

<p>
Don't forget those 300 data points. We'd have to fit the whole thing with them!
</p>

</aside>
</section>
<section id="slide-orgheadline24">
<h3 id="orgheadline24">Resulting Posterior</h3>

<div class="figure">
<p><img src="kruschke-hlr-posterior.png" alt="kruschke-hlr-posterior.png" />
</p>
</div>

<aside class="notes">
<p>
Kruschke calculates this model using Stan, and gets those quite <b>rough</b> histograms, that show
the marginal probabilities of the 4 dimensions. Black line denotes the region that contains 95% probability
(note: NOT the same as confidence intervals).
</p>

<p>
We see that &beta;<sub>0</sub> is between -200 and -80, &beta;<sub>1</sub> between 3.5 and 5.3, &sigma; around 24 and &nu; around 5.
</p>

<p>
It took Kruschke <b>several hundred seconds</b> to complete this calculation with Stan/JAGS!
</p>

</aside>

</section>
<section id="slide-orgheadline25">
<h3 id="orgheadline25">Read the data</h3>
<div class="org-src-container">

<pre  class="src src-clojure"><span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">defn</span> <span style="color: #ff1493; font-weight: bold;">read-data</span> <span style="color: #006400;">[</span>in-file<span style="color: #006400;">]</span>
  <span style="color: #006400;">(</span><span style="color: #4c83ff; font-weight: bold;">loop</span> <span style="color: #ff1493;">[</span>c 0 data <span style="color: #ffff00;">(</span>drop 1 <span style="color: #00ff00;">(</span><span style="color: #afd8af; font-weight: bold;">csv</span><span style="color: #d3d3d3; background-color: #000000;">/</span>read-csv in-file<span style="color: #00ff00;">)</span><span style="color: #ffff00;">)</span> hw <span style="color: #ffff00;">(</span>transient <span style="color: #00ff00;">[]</span><span style="color: #ffff00;">)</span><span style="color: #ff1493;">]</span>
    <span style="color: #ff1493;">(</span><span style="color: #4c83ff; font-weight: bold;">if</span> data
      <span style="color: #ffff00;">(</span><span style="color: #4c83ff; font-weight: bold;">let</span> <span style="color: #00ff00;">[</span><span style="color: #add8e6;">[</span>_ h w<span style="color: #add8e6;">]</span> <span style="color: #add8e6;">(</span>first data<span style="color: #add8e6;">)</span><span style="color: #00ff00;">]</span>
        <span style="color: #00ff00;">(</span><span style="color: #4c83ff; font-weight: bold;">recur</span> <span style="color: #add8e6;">(</span>inc c<span style="color: #add8e6;">)</span> <span style="color: #add8e6;">(</span>next data<span style="color: #add8e6;">)</span>
               <span style="color: #add8e6;">(</span><span style="color: #4c83ff; font-weight: bold;">-&gt;</span> hw <span style="color: #ffa500;">(</span>conj! <span style="color: #6a5acd;">(</span>double <span style="color: #d3d3d3;">(</span>read-string h<span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span>
                   <span style="color: #ffa500;">(</span>conj! <span style="color: #6a5acd;">(</span>double <span style="color: #d3d3d3;">(</span>read-string w<span style="color: #d3d3d3;">)</span><span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span><span style="color: #add8e6;">)</span><span style="color: #00ff00;">)</span><span style="color: #ffff00;">)</span>
      <span style="color: #ffff00;">(</span>op <span style="color: #00ff00;">[</span>c<span style="color: #00ff00;">]</span> <span style="color: #00ff00;">(</span>persistent! hw<span style="color: #00ff00;">)</span><span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>

<span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">def</span> <span style="color: #ff69b4; font-weight: bold;">params-300</span> <span style="color: #006400;">(</span>sv <span style="color: #ff1493;">(</span>read-data <span style="color: #ffff00;">(</span>slurp <span style="color: #00ff00;">(</span><span style="color: #afd8af; font-weight: bold;">io</span><span style="color: #d3d3d3; background-color: #000000;">/</span>file <span style="color: #61CE3C;">"ht-wt-data-300.csv"</span><span style="color: #00ff00;">)</span><span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>
</pre>
</div>

<aside class="notes">
<p>
Let's do the same thing with Bayadera. I'm writing quite plain and low-level code here
to show that even without additional Clojure abstractions, it is still not more complex
than the alternative R/Python models.
</p>

<p>
First, we use <b>plain Clojure</b> to read the data and put it into a vector of parameters that
we will feed to Bayadera's engine.
</p>

</aside>
<pre class="example">
#'user/read-data#'user/params-300
</pre>

</section>
<section id="slide-orgheadline26">
<h3 id="orgheadline26">Create a custom model</h3>
<div class="org-src-container">

<pre  class="src src-clojure"><span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">def</span> <span style="color: #ff69b4; font-weight: bold;">rlr-source</span> <span style="color: #006400;">(</span>slurp <span style="color: #ff1493;">(</span><span style="color: #afd8af; font-weight: bold;">io</span><span style="color: #d3d3d3; background-color: #000000;">/</span>file <span style="color: #61CE3C;">"robust-linear-regression.h"</span><span style="color: #ff1493;">)</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>

<span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">def</span> <span style="color: #ff69b4; font-weight: bold;">rlr-prior</span>
  <span style="color: #006400;">(</span>cl-distribution-model <span style="color: #ff1493;">[</span><span style="color: #ffff00;">(</span><span style="color: #96CBFE;">:gaussian</span> source-library<span style="color: #ffff00;">)</span>
                          <span style="color: #ffff00;">(</span><span style="color: #96CBFE;">:uniform</span> source-library<span style="color: #ffff00;">)</span>
                          <span style="color: #ffff00;">(</span><span style="color: #96CBFE;">:exponential</span> source-library<span style="color: #ffff00;">)</span>
                          <span style="color: #ffff00;">(</span><span style="color: #96CBFE;">:t</span> source-library<span style="color: #ffff00;">)</span>
                          rlr-source<span style="color: #ff1493;">]</span>
                         <span style="color: #96CBFE;">:name</span> <span style="color: #61CE3C;">"rlr"</span> <span style="color: #96CBFE;">:mcmc-logpdf</span> <span style="color: #61CE3C;">"rlr_mcmc_logpdf"</span>
                         <span style="color: #96CBFE;">:params-size</span> 7 <span style="color: #96CBFE;">:dimension</span> 4<span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>

<span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">defn</span> <span style="color: #ff1493; font-weight: bold;">rlr-likelihood</span> <span style="color: #006400;">[</span>n<span style="color: #006400;">]</span>
  <span style="color: #006400;">(</span>cl-likelihood-model rlr-source <span style="color: #96CBFE;">:name</span> <span style="color: #61CE3C;">"rlr"</span> <span style="color: #96CBFE;">:params-size</span> n<span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>
</pre>
</div>

<aside class="notes">
<p>
We use some Bayadera's built-in models of common distribution and combine them
into GPU code of this specific hierarchical model.
</p>

<p>
Although we use some stock distribution, the actual hierarchy is specific for this model.
We have to specify that in some <b>straightforward</b> OpenCL C code.
</p>

</aside>

<pre class="example">
#'user/rlr-source#'user/rlr-prior#'user/rlr-likelihood
</pre>

</section>
<section id="slide-orgheadline27">
<h3 id="orgheadline27">Model's likelihood function</h3>
<div class="org-src-container">

<pre  class="src src-c"><span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff1493; font-weight: bold;">rlr_loglik</span>(<span style="color: #afd8af; font-weight: bold;">__constant</span> <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #ff69b4; font-weight: bold;">REAL</span>* params, <span style="color: #afd8af; font-weight: bold;">REAL</span>* <span style="color: #ff69b4; font-weight: bold;">x</span>) {
    <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff69b4; font-weight: bold;">nu</span> = x[0];
    <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff69b4; font-weight: bold;">b0</span> = x[1];
    <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff69b4; font-weight: bold;">b1</span> = x[2];
    <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff69b4; font-weight: bold;">sigma</span> = x[3];
    <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #afd8af; font-weight: bold;">uint</span> <span style="color: #ff69b4; font-weight: bold;">n</span> = (<span style="color: #afd8af; font-weight: bold;">uint</span>)params[0];
    <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #afd8af; font-weight: bold;">bool</span> <span style="color: #ff69b4; font-weight: bold;">valid</span> = (0.0f &lt; nu) &amp;&amp; (0.0f &lt; sigma);
    <span style="color: #4c83ff; font-weight: bold;">if</span> (valid) {
        <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff69b4; font-weight: bold;">scale</span> = t_log_scale(nu, sigma);
        <span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff69b4; font-weight: bold;">res</span> = 0.0;
        <span style="color: #4c83ff; font-weight: bold;">for</span> (<span style="color: #afd8af; font-weight: bold;">uint</span> <span style="color: #ff69b4; font-weight: bold;">i</span> = 0; i &lt; n; i = i+2) {
            res += t_log_unscaled(nu,
                                  b0 + b1 * params[i+1],
                                  sigma, params[i+2])
                + scale;
        }
        <span style="color: #4c83ff; font-weight: bold;">return</span> res;
    }
    <span style="color: #4c83ff; font-weight: bold;">return</span> NAN;
}
</pre>
</div>

<aside class="notes">
<p>
Here is the likelihood function corresponding to the model from the diagrams. We use
built-in t<sub>log</sub><sub>unscaled</sub> function and feed it the appropriate parameters, and do this in
the loop for all 300 data points.
</p>

</aside>

</section>
<section id="slide-orgheadline28">
<h3 id="orgheadline28">Model's prior function</h3>
<div class="org-src-container">

<pre  class="src src-c"><span style="color: #afd8af; font-weight: bold;">REAL</span> <span style="color: #ff1493; font-weight: bold;">rlr_logpdf</span>(<span style="color: #afd8af; font-weight: bold;">__constant</span> <span style="color: #4c83ff; font-weight: bold;">const</span> <span style="color: #ff69b4; font-weight: bold;">REAL</span>* params, <span style="color: #afd8af; font-weight: bold;">REAL</span>* <span style="color: #ff69b4; font-weight: bold;">x</span>) {
    <span style="color: #4c83ff; font-weight: bold;">return</span> exponential_log(params[0], x[0] - 1)
        + gaussian_log(params[1], params[2], x[1])
        + gaussian_log(params[3], params[4], x[2])
        + uniform_log(params[5], params[6], x[3]);

}
</pre>
</div>

<aside class="notes">
<p>
Prior is much easier. You can recognize that distributions from the hierarchical
diagram are simply combined to compute the probability for any combination of parameters
that we give it.
</p>

<p>
The user does not call those functions explicitly. They are just specific parts of the
engine that will be automagically compiled into GPU code by Bayadera.
</p>

<p>
In the coin model, we did not have to do this step, because we could just reuse stock
models that are already built in Bayadera. Once we created <b>this</b> model, we can also reuse
it in a similar way.
</p>

</aside>
</section>
<section id="slide-orgheadline29">
<h3 id="orgheadline29">Running the inference on the GPU</h3>
<div class="org-src-container">

<pre  class="src src-clojure"><span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">def</span> <span style="color: #ff69b4; font-weight: bold;">result</span> <span style="color: #006400;">(</span>atom
<span style="color: #ff1493;">(</span>with-default-bayadera
  <span style="color: #ffff00;">(</span>with-release
    <span style="color: #00ff00;">[</span>prior <span style="color: #add8e6;">(</span>distribution rlr-prior<span style="color: #add8e6;">)</span>
     prior-dist <span style="color: #add8e6;">(</span>prior <span style="color: #ffa500;">(</span>sv 10 -100 100 5 10 0.001 1000<span style="color: #ffa500;">)</span><span style="color: #add8e6;">)</span>

     post <span style="color: #add8e6;">(</span>posterior <span style="color: #61CE3C;">"rlr_300"</span> <span style="color: #ffa500;">(</span>rlr-likelihood <span style="color: #6a5acd;">(</span>dim params-300<span style="color: #6a5acd;">)</span><span style="color: #ffa500;">)</span> prior-dist<span style="color: #add8e6;">)</span>
     post-dist <span style="color: #add8e6;">(</span>post params-300<span style="color: #add8e6;">)</span>
     post-sampler <span style="color: #add8e6;">(</span>sampler post-dist <span style="color: #ffa500;">{</span><span style="color: #96CBFE;">:limits</span> <span style="color: #6a5acd;">(</span>sge 2 4 <span style="color: #d3d3d3;">[</span>1 10 -400 100 0 20 0.01 100<span style="color: #d3d3d3;">]</span><span style="color: #6a5acd;">)</span><span style="color: #ffa500;">}</span><span style="color: #add8e6;">)</span><span style="color: #00ff00;">]</span>

    <span style="color: #00ff00;">(</span>mix! post-sampler <span style="color: #add8e6;">{</span><span style="color: #96CBFE;">:step</span> 384<span style="color: #add8e6;">}</span><span style="color: #00ff00;">)</span>

    <span style="color: #00ff00;">(</span>histogram! post-sampler 1000<span style="color: #00ff00;">)</span><span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>
</pre>
</div>


<aside class="notes">
<p>
Running the inference is even simpler than in the coin example.
</p>

<p>
The new thing is that the sample itself is multidimensional, so we must make histogram to
calculate marginal probabilities that we are interested in. But, this is something that Bayadera
can do blazingly fast with its built in functions.
</p>

</aside>
</section>
<section id="slide-orgheadline30">
<h3 id="orgheadline30">Generate posterior diagrams</h3>
<div class="org-src-container">

<pre  class="src src-clojure"><span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">defn</span> <span style="color: #ff1493; font-weight: bold;">var-plot</span> <span style="color: #006400;">[]</span> <span style="color: #006400;">(</span>plot2d <span style="color: #ff1493;">(</span><span style="color: #afd8af; font-weight: bold;">qa</span><span style="color: #d3d3d3; background-color: #000000;">/</span>current-applet<span style="color: #ff1493;">)</span> <span style="color: #ff1493;">{</span><span style="color: #96CBFE;">:width</span> 480 <span style="color: #96CBFE;">:height</span> 240<span style="color: #ff1493;">}</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>

<span style="color: #8b0000;">(</span><span style="color: #4c83ff; font-weight: bold;">defn</span> <span style="color: #ff1493; font-weight: bold;">setup</span> <span style="color: #006400;">[]</span>
  <span style="color: #006400;">(</span><span style="color: #afd8af; font-weight: bold;">q</span><span style="color: #d3d3d3; background-color: #000000;">/</span>background 0<span style="color: #006400;">)</span>
  <span style="color: #006400;">(</span><span style="color: #afd8af; font-weight: bold;">q</span><span style="color: #d3d3d3; background-color: #000000;">/</span>image <span style="color: #ff1493;">(</span>show <span style="color: #ffff00;">(</span>render-histogram <span style="color: #00ff00;">(</span>var-plot<span style="color: #00ff00;">)</span> @result 1<span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span> 0 0<span style="color: #006400;">)</span>
  <span style="color: #006400;">(</span><span style="color: #afd8af; font-weight: bold;">q</span><span style="color: #d3d3d3; background-color: #000000;">/</span>image <span style="color: #ff1493;">(</span>show <span style="color: #ffff00;">(</span>render-histogram <span style="color: #00ff00;">(</span>var-plot<span style="color: #00ff00;">)</span> @result 2<span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span> 480 0<span style="color: #006400;">)</span>
  <span style="color: #006400;">(</span><span style="color: #afd8af; font-weight: bold;">q</span><span style="color: #d3d3d3; background-color: #000000;">/</span>image <span style="color: #ff1493;">(</span>show <span style="color: #ffff00;">(</span>render-histogram <span style="color: #00ff00;">(</span>var-plot<span style="color: #00ff00;">)</span> @result 3<span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span> 0 260<span style="color: #006400;">)</span>
  <span style="color: #006400;">(</span><span style="color: #afd8af; font-weight: bold;">q</span><span style="color: #d3d3d3; background-color: #000000;">/</span>image <span style="color: #ff1493;">(</span>show <span style="color: #ffff00;">(</span>render-histogram <span style="color: #00ff00;">(</span>var-plot<span style="color: #00ff00;">)</span> @result 0<span style="color: #ffff00;">)</span><span style="color: #ff1493;">)</span> 480 260<span style="color: #006400;">)</span>

  <span style="color: #006400;">(</span><span style="color: #afd8af; font-weight: bold;">q</span><span style="color: #d3d3d3; background-color: #000000;">/</span>save <span style="color: #61CE3C;">"robust-linear-regression.png"</span><span style="color: #006400;">)</span><span style="color: #8b0000;">)</span>

#_<span style="color: #8b0000; font-style: italic;">(</span><span style="color: #8B8989; font-style: italic;">q/defsketch diagrams :renderer :p2d :size :fullscreen</span>
<span style="color: #8B8989; font-style: italic;">    :setup setup :middleware </span><span style="color: #006400; font-style: italic;">[</span><span style="color: #8B8989; font-style: italic;">pause-on-error</span><span style="color: #006400; font-style: italic;">]</span><span style="color: #8b0000; font-style: italic;">)</span>
</pre>
</div>

</section>
<section id="slide-orgheadline31">
<h3 id="orgheadline31">Fine grained histograms!</h3>
<p>
Posteriors: &beta;<sub>0</sub>, &beta;<sub>1,&sigma;</sub>, and &nu;
<img src="robust-linear-regression.png" alt="robust-linear-regression.png" />
</p>
<aside class="notes">
<p>
The diagram.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orgheadline34">
<h2 id="orgheadline34">How fast is it?</h2>
<ul>
<li>Bayadera
<ul>
<li>61,208,576 samples in 267 ms.</li>
<li>4.36 ns per (computationally heavy) sample</li>
<li>very precise histogram</li>

</ul></li>
<li>JAGS/Stan (state-of-the-art bayesian C++ tools)
<ul>
<li>20,000 samples in 180/485 seconds</li>
<li>9 ms per sample</li>
<li>rough histogram</li>

</ul></li>
<li><font color = "red">2,000,000 &times;</font> faster per sample</li>
<li>more precise results, <font color = "red">1000 &times;</font> faster</li>

</ul>

<aside class="notes">
<p>
What I really wanted to show you is this: despite <b>MCMC being sequential by nature</b>,
Bayadera runs circles around the best state of the art Bayesian environments!
</p>

<p>
For this particular analysis, Bayadera took 60 M samples during the
exploration of that complex multi-dimensional space in just 267 ms, which is 4 ns per sample!
</p>

<p>
JAGS and Stan, two best popular R/Python tools implemented in C++ spent 180/485 seconds
to get 20K samples.
</p>

<p>
This is 2 million times less per sample. However, it is not fair to compare those
directly, since Stan tries to spend more time on getting supposedly better convergence.
</p>

<p>
But, when we compare the <b>whole analysis</b>, we can see that Bayadera was roughly
1000 times faster, and yet produced a better result with much more precise histograms
(not shown here)!
</p>

</aside>

</section>
<section id="slide-orgheadline33">
<h3 id="orgheadline33">In real life</h3>
<ul>
<li>1 second vs a couple of hours</li>
<li>1 minute vs several days!</li>
<li>1 hour vs couple months/ a year</li>

</ul>

<aside class="notes">
<p>
Now, you might think - but who cares if something is 1000 times faster or slower?
So what if it takes a couple of milliseconds instead of a few nanoseconds? Both are
faster than the blink of an eye.
</p>

<p>
Well, considering number of seconds in a day, hours in a month, etc&#x2026;
This is a difference of having to wait 1 second instead of an hour,
or 1 minute instead of a few days, or 1 hour instead of months or even a year!
</p>

</aside>


</section>
</section>
<section>
<section id="slide-orgheadline35">
<h2 id="orgheadline35">Thank You</h2>
<p>
The presentation can be reached through my blog:
</p>
<ul>
<li><a href="http://dragan.rocks">http://dragan.rocks</a></li>

</ul>

<p>
Find more at:
</p>
<ul>
<li><a href="http://bayadera.uncomplicate.org">http://bayadera.uncomplicate.org</a></li>
<li><a href="http://clojurecl.uncomplicate.org">http://clojurecl.uncomplicate.org</a></li>
<li><a href="http://neanderthal.uncomplicate.org">http://neanderthal.uncomplicate.org</a></li>
<li><a href="http://fluokitten.uncomplicate.org">http://fluokitten.uncomplicate.org</a></li>

</ul>
</section>
</section>
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/lib/js/head.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
